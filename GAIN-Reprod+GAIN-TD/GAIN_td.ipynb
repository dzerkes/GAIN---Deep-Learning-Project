{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "National Technical University of Athens  \n",
    "School of Electrical and Computer Engineering   \n",
    "Data Science and Machine Learning   \n",
    "  \n",
    "\"Deep Learning Project\"  \n",
    "   \n",
    "Creators:  \n",
    "Zerkelidis Dimitris   \n",
    "Kaiktzoglou Maria  \n",
    "Trivyza Marilia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAIN: Missing Data Imputation using Generative Adversarial Nets\n",
    "\n",
    "Reference: Jinsung Yoon, James Jordon, and Mihaela van der Schaar, \"GAIN: Missing Data Imputation using Generative Adversarial Nets\", In International Conference on Machine Learning (ICML), 2018.  \n",
    "\n",
    "Adapted from the original implementation in tensorflow: https://github.com/jsyoon0823/GAIN  \n",
    "\n",
    "\n",
    "#### breast, spam, letter, credit, news Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN-TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from numpy import linalg as LA\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False # set it to True to use GPU and False to use CPU\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Parameters\n",
    "\n",
    "Mini batch size: 64   \n",
    "Missing rate: 0.2  \n",
    "Hint rate: 0.9  \n",
    "Loss Hyperparameters: 10  \n",
    "Train Rate: 0.8  \n",
    "Learning Rate: 0.001  \n",
    "Epochs: 10000   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Initialize System Parameters\n",
    "def init_params(mb_size=64, p_miss=0.2, p_hint=0.9, alpha=10, train_rate=0.8, learning_rate=0.001, epochs=10000): \n",
    "    # Mini batch size\n",
    "    mb_size = mb_size\n",
    "    # Missing rate\n",
    "    p_miss = p_miss\n",
    "    # Hint rate\n",
    "    p_hint = p_hint\n",
    "    # Loss Hyperparameters\n",
    "    alpha = alpha\n",
    "    # Train Rate\n",
    "    train_rate = train_rate\n",
    "    # Learning Rate\n",
    "    learning_rate = learning_rate\n",
    "    # Epochs\n",
    "    epochs = epochs\n",
    "    \n",
    "    return mb_size, p_miss, p_hint, alpha, train_rate, learning_rate, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size, p_miss, p_hint, alpha, train_rate, learning_rate, epochs = init_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN-TD Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, Dim, H_Dim1, H_Dim2):\n",
    "        super(NetD, self).__init__()\n",
    "        \n",
    "        self.fc0 = torch.nn.Linear(Dim * 1, H_Dim1) # without Hint \n",
    "        self.fc1 = torch.nn.Linear(Dim * 2, H_Dim1) # with Hint\n",
    "        self.fc2 = torch.nn.Linear(H_Dim1, H_Dim2)\n",
    "        self.fc3 = torch.nn.Linear(H_Dim2, H_Dim1)\n",
    "        self.fc4 = torch.nn.Linear(H_Dim1, Dim)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        layers = [self.fc0, self.fc1, self.fc2, self.fc3, self.fc4]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "    def forward(self, x, h, hint): \n",
    "        if hint == False:\n",
    "            inp = x # Data without Hint\n",
    "            out = self.tanh(self.fc0(inp))\n",
    "        else: \n",
    "            inp = torch.cat((x, h), dim=1)  # Hint + Data Concatenate\n",
    "            out = self.tanh(self.fc1(inp))\n",
    "        \n",
    "        out = self.tanh(self.fc2(out))\n",
    "        out = self.tanh(self.fc3(out))\n",
    "        out = self.sigmoid(self.fc4(out)) # [0,1] Probability Output\n",
    "        return out\n",
    "\n",
    "    \n",
    "# Generator\n",
    "class NetG(torch.nn.Module):\n",
    "    def __init__(self, Dim, H_Dim1, H_Dim2):\n",
    "        super(NetG, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(Dim * 2, H_Dim1) \n",
    "        self.fc2 = torch.nn.Linear(H_Dim1, H_Dim2)\n",
    "        self.fc3 = torch.nn.Linear(H_Dim2, H_Dim1)\n",
    "        self.fc4 = torch.nn.Linear(H_Dim1, Dim)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        layers = [self.fc1, self.fc2, self.fc3, self.fc4]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "    def forward(self, x, m): \n",
    "        inp = torch.cat((x, m), dim=1)\n",
    "        out = self.tanh(self.fc1(inp))\n",
    "        out = self.tanh(self.fc2(out))\n",
    "        out = self.tanh(self.fc3(out))\n",
    "        out = self.sigmoid(self.fc4(out)) # [0,1] Probability Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint Vector Generation\n",
    "def sample_M(m, n, p):                         # e.g. m=mb_size , n=Dim, p=1-p_hint =0.1\n",
    "    A = np.random.uniform(0., 1., size=[m, n]) # size of mb_size X Dim -> values between 0 to 1\n",
    "    B = A > p                                  # if A value bigger than 0.1 then True and C=1 else False and C=0\n",
    "    C = 1. * B\n",
    "    return C                                   # C is shape mb_size X Dim\n",
    "\n",
    "\n",
    "# Random sample generator for Z\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(0., 0.01, size = [m, n])   \n",
    "\n",
    "\n",
    "# Mini-batch generation\n",
    "def sample_idx(m, n):              \n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(netG, netD, M, X, H, hint): \n",
    "    # Generator\n",
    "    G_sample = netG(X, M)\n",
    "    # Combine with original data\n",
    "    Hat_New_X = X * M + G_sample * (1-M)\n",
    "    # Discriminator\n",
    "    D_prob = netD(Hat_New_X, H, hint)\n",
    "    # Loss\n",
    "    D_loss = -torch.mean(M * torch.log(D_prob + 1e-8) + (1-M) * torch.log(1. - D_prob + 1e-8))\n",
    "    return D_loss\n",
    "\n",
    "\n",
    "def generator_loss(netG, netD, X, M, New_X, H, Lg, Lm, hint, alpha=alpha):\n",
    "    # %% Structure\n",
    "    # Generator\n",
    "    G_sample = netG(New_X, M)\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "    # Discriminator\n",
    "    D_prob = netD(Hat_New_X, H, hint)\n",
    "\n",
    "    # Loss\n",
    "    G_loss1 = -torch.mean((1-M) * torch.log(D_prob + 1e-8))\n",
    "    # MSE loss finds the difference between New_X and Generators Sample only on the real values\n",
    "    # Because i want the real values to be as close as they can be to the New_X. I care only for the imputed values\n",
    "    MSE_train_loss = torch.mean((M * New_X - M * G_sample)**2) / torch.mean(M)\n",
    "    \n",
    "    # G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "    if Lm == False:\n",
    "        G_loss = G_loss1 \n",
    "    elif Lg == False:\n",
    "        G_loss = alpha * MSE_train_loss\n",
    "    elif (Lm == True) and (Lg == True):\n",
    "        G_loss = G_loss1 + alpha * MSE_train_loss\n",
    "\n",
    "    # MSE Performance metric\n",
    "    # The difference between real data , X and the Imputed Data\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return G_loss, MSE_train_loss, MSE_test_loss\n",
    "\n",
    "\n",
    "def test_loss(netG, netD, X, M, New_X):\n",
    "    # %% Structure\n",
    "    # Generator\n",
    "    G_sample = netG(New_X, M)\n",
    "\n",
    "    # MSE Performance metric\n",
    "    # The difference between real data , X and the Imputed Data\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return MSE_test_loss, G_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializations(dataset_file, size=False, lnth=0, impute=False): \n",
    "    # Data generation\n",
    "    Data = np.loadtxt(dataset_file, delimiter=\",\", skiprows=1)\n",
    "    # Shuffle Data\n",
    "    np.random.seed(1234)\n",
    "    np.random.shuffle(Data)\n",
    "    \n",
    "    if size==True:\n",
    "        Data = Data[:lnth] # for data set size vs rmse plot\n",
    "    \n",
    "    if impute==True:\n",
    "        Data_y = Data[:,Data.shape[1]-1]\n",
    "        Data = Data[:,:Data.shape[1]-1]\n",
    "    \n",
    "    # Parameters\n",
    "    No = len(Data)\n",
    "    Dim = len(Data[0,:])\n",
    "\n",
    "    # Hidden state dimensions\n",
    "    H_Dim1 = Dim\n",
    "    H_Dim2 = Dim // 2\n",
    "\n",
    "    # Normalization (0 to 1)\n",
    "    # z = (x-min) / (X_max + ε)\n",
    "    # ε is used to avoid division by zero\n",
    "    Min_Val = np.zeros(Dim)\n",
    "    Max_Val = np.zeros(Dim)\n",
    "\n",
    "    for i in range(Dim):\n",
    "        Min_Val[i] = np.min(Data[:,i])\n",
    "        Data[:,i] = Data[:,i] - np.min(Data[:,i])\n",
    "        Max_Val[i] = np.max(Data[:,i])\n",
    "        Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)    \n",
    "    \n",
    "    # %% Missing introducing\n",
    "    p_miss_vec = p_miss * np.ones((Dim,1)) # p_miss ... Dim x 1 size, Dim is number of columns of dataset \n",
    "\n",
    "    Missing = np.zeros((No,Dim))           # zero ... Size of dataset No, Dim , No is the rows\n",
    "    # Mask Vector Generation\n",
    "    for i in range(Dim):\n",
    "        A = np.random.uniform(0., 1., size = [len(Data),]) # A is size No x 1\n",
    "        B = A > p_miss_vec[i]       # B is size No x 1, If a value is False, it is a missing data point\n",
    "        Missing[:,i] = 1.*B         # No x Dim, 1 * False = 0 and 1 * True = 1\n",
    "    \n",
    "    # %% Train Test Division\n",
    "    idx = np.random.permutation(No) # number of No values. permutation of numbers from 0 - No\n",
    "    Train_No = int(No * train_rate) # No.of Rows * train_rate\n",
    "    Test_No = No - Train_No         # 1 - Train_No \n",
    "    \n",
    "    if impute==False:\n",
    "        return Data, No, Dim, H_Dim1, H_Dim2, Min_Val, Max_Val, p_miss_vec, Missing, idx, Train_No, Test_No\n",
    "    else:\n",
    "        return Data, Data_y, No, Dim, H_Dim1, H_Dim2, Min_Val, Max_Val, p_miss_vec, Missing, idx, Train_No, Test_No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAIN(netG, netD, Dim, trainX, trainM, optimG, optimD, cv=True, Lg=True, Lm=True, hint=True, \n",
    "              alpha=alpha, epochs=epochs, printLoss=False):\n",
    "    # %% Training\n",
    "    # %% Start Iterations\n",
    "    for it in tqdm(range(epochs)):\n",
    "        # %% Inputs\n",
    "        if cv == False:\n",
    "            mb_idx = sample_idx(Train_No, mb_size)    # choose indexes\n",
    "            X_mb = trainX[mb_idx,:]  \n",
    "\n",
    "            Z_mb = sample_Z(mb_size, Dim)             # uniform 0 to 0.01\n",
    "            M_mb = trainM[mb_idx, :]                  # mini batch for Missing, 1 and 0 matrix\n",
    "            H_mb1 = sample_M(mb_size, Dim, 1-p_hint)  # random 0 or 1 with 1 if prob > 0.1\n",
    "            H_mb = M_mb * H_mb1 # Hint vector final -> 0 on all Missing values and also to some known\n",
    "                                # because of a small probability that H_mb1 having 0 values, \n",
    "                                # but most known data points are 1\n",
    "            \n",
    "            New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce \n",
    "        else:\n",
    "            mb_idx = sample_idx(len(trainX), mb_size)\n",
    "            X_mb = trainX[mb_idx, :]\n",
    "            \n",
    "            Z_mb = sample_Z(mb_size, Dim)\n",
    "            M_mb = trainM[mb_idx, :] \n",
    "            H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "            H_mb = M_mb * H_mb1\n",
    "            \n",
    "            New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
    "        \n",
    "        if use_gpu is True:\n",
    "            X_mb = torch.tensor(X_mb, device='cuda').float()\n",
    "            M_mb = torch.tensor(M_mb, device='cuda').float()\n",
    "            H_mb = torch.tensor(H_mb, device='cuda').float()\n",
    "            New_X_mb = torch.tensor(New_X_mb, device='cuda').float()\n",
    "        else:\n",
    "            X_mb = torch.tensor(X_mb).float()\n",
    "            M_mb = torch.tensor(M_mb).float()\n",
    "            H_mb = torch.tensor(H_mb).float()\n",
    "            New_X_mb = torch.tensor(New_X_mb).float()\n",
    "\n",
    "        # Train D\n",
    "        optimD.zero_grad() \n",
    "        D_loss = discriminator_loss(netG, netD, M=M_mb, X=New_X_mb, H=H_mb, hint=hint)\n",
    "        D_loss.backward()\n",
    "        optimD.step()\n",
    "\n",
    "        # Train G\n",
    "        optimG.zero_grad() \n",
    "        G_loss, G_mse_loss, G_mse_test = generator_loss(netG, netD, X=X_mb, M=M_mb, New_X=New_X_mb, H=H_mb, \n",
    "                                                        Lg=Lg, Lm=Lm, hint=hint, alpha=alpha)\n",
    "        G_loss.backward()\n",
    "        optimG.step()\n",
    "        \n",
    "        # %% Intermediate Losses\n",
    "        if printLoss==True:\n",
    "            if it % 100 == 0:\n",
    "                print('Iter: {}'.format(it),end='\\t')\n",
    "                print('Train_loss: {:.4}'.format(G_mse_loss),end='\\t')\n",
    "                print('Test_loss: {:.4}'.format(G_mse_test),end='\\t')\n",
    "                print('D_loss: {:.4}'.format(D_loss))\n",
    "\n",
    "\n",
    "def testGAIN(netG, netD, Dim, testM, testX, cv=True, impute=False):\n",
    "    if cv == False:\n",
    "        Z_mb = sample_Z(Test_No, Dim) \n",
    "    else:\n",
    "        Z_mb = sample_Z(len(testM), Dim) \n",
    "        \n",
    "    M_mb = testM # all test samples together\n",
    "    X_mb = testX\n",
    "\n",
    "    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "\n",
    "    # make the tensors\n",
    "    if use_gpu is True:\n",
    "        X_mb = torch.tensor(X_mb, device='cuda').float()\n",
    "        M_mb = torch.tensor(M_mb, device='cuda').float()\n",
    "        New_X_mb = torch.tensor(New_X_mb, device='cuda').float()\n",
    "    else:\n",
    "        X_mb = torch.tensor(X_mb).float()\n",
    "        M_mb = torch.tensor(M_mb).float()\n",
    "        New_X_mb = torch.tensor(New_X_mb).float()\n",
    "\n",
    "    MSE_final, Sample = test_loss(netG, netD, X=X_mb, M=M_mb, New_X=New_X_mb)\n",
    "\n",
    "    \"\"\"print('Final Test RMSE: ' + str(np.sqrt(MSE_final.item())))\n",
    "    print()\"\"\"\n",
    "    \n",
    "    if impute==False:\n",
    "        return np.sqrt(MSE_final.item())\n",
    "    else:\n",
    "        return Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(dataset_file, Lg=True, Lm=True, hint=True, alpha=alpha, epochs=epochs, \n",
    "               learning_rate=learning_rate, size=False, lnth=0, impute=False, printLoss=False): \n",
    "    # %% Initializations    \n",
    "    if impute==False:\n",
    "        if size==False:\n",
    "            Data, No, Dim, H_Dim1, H_Dim2, Min_Val, Max_Val, p_miss_vec, Missing, idx, Train_No, Test_No = initializations(dataset_file)\n",
    "        else:\n",
    "            Data, No, Dim, H_Dim1, H_Dim2, Min_Val, Max_Val, p_miss_vec, Missing, idx, Train_No, Test_No = initializations(dataset_file, size=size, lnth=lnth)           \n",
    "    else:\n",
    "        Data, Data_y, No, Dim, H_Dim1, H_Dim2, Min_Val, Max_Val, p_miss_vec, Missing, idx, Train_No, Test_No = initializations(dataset_file, impute=impute)\n",
    "    \n",
    "    # %% Cross Validation\n",
    "    kf = KFold(n_splits=5)\n",
    "    experiment_rmse_mean =[]\n",
    "    experiment_rmse_std=[]\n",
    "    auroc_mean = []\n",
    "    \n",
    "    for i in range(2): # Number of experiments\n",
    "        if impute==False:  \n",
    "            rmse_cv_scores=[]\n",
    "            for train_index, test_index in kf.split(Data):\n",
    "\n",
    "                # Train / Test Features\n",
    "                trainX, testX = Data[train_index], Data[test_index]\n",
    "                # Train / Test Missing Indicators\n",
    "                trainM, testM = Missing[train_index], Missing[test_index]\n",
    "\n",
    "                if impute==True:\n",
    "                    trainY, testY = Data_y[train_index], Data_y[test_index]\n",
    "\n",
    "                # %% Init Network \n",
    "                if use_gpu is True:\n",
    "                    netD = NetD(Dim, H_Dim1, H_Dim2).cuda()\n",
    "                    netG = NetG(Dim, H_Dim1, H_Dim2).cuda()\n",
    "                else:\n",
    "                    netD = NetD(Dim, H_Dim1, H_Dim2)\n",
    "                    netG = NetG(Dim, H_Dim1, H_Dim2)\n",
    "                    \n",
    "                # Optimizers\n",
    "                optimD = torch.optim.Adam(netD.parameters(), lr=learning_rate) # discriminator optimizer\n",
    "                optimG = torch.optim.Adam(netG.parameters(), lr=learning_rate) # generator optimizer\n",
    "\n",
    "                # %% Training\n",
    "                trainGAIN(netG, netD, Dim, trainX, trainM, optimG, optimD, Lg=Lg, Lm=Lm, hint=hint, \n",
    "                          alpha=alpha, epochs=epochs, printLoss=printLoss)\n",
    "\n",
    "                # %% Testing\n",
    "                if impute==False:\n",
    "                    rmse_cv_scores.append(testGAIN(netG, netD, Dim, testM, testX))\n",
    "                else:\n",
    "                    # Impute train and test and then fit on logistic\n",
    "                    trainX = testGAIN(netG, netD, Dim, trainX, trainM, impute=impute)\n",
    "                    testX  = testGAIN(netG, netD, Dim, testX, testM, impute=impute)\n",
    "\n",
    "                    # Train a logistic Regression\n",
    "                    clf = LogisticRegression(random_state=0,solver='lbfgs', \n",
    "                                             max_iter=2000).fit(trainX.detach().numpy(),trainY)\n",
    "                    preds = clf.predict(testX.detach().numpy())\n",
    "\n",
    "                    # print(\"roc_auc_score: {}\".format(roc_auc_score(testY, preds)))\n",
    "                    fpr, tpr, thresholds = roc_curve(testY, preds)\n",
    "                    auroc_scores.append(auc(fpr, tpr))\n",
    "                    \n",
    "            # %% Calculate avg_RMSE on test\n",
    "            mean = sum(rmse_cv_scores) / len(rmse_cv_scores)\n",
    "            print(\"Average RMSE score: {}\".format(mean))\n",
    "\n",
    "            # %% Calculate the std on test\n",
    "            variance = sum((rmse - mean) ** 2 for rmse in rmse_cv_scores) / len(rmse_cv_scores) \n",
    "            std = np.sqrt(variance)\n",
    "            print(\"Standard Deviation of RMSE score: {}\".format(std))\n",
    "\n",
    "            experiment_rmse_mean.append(mean)\n",
    "            experiment_rmse_std.append(std)\n",
    "        else:\n",
    "            auroc_scores=[]\n",
    "            for train_index, test_index in kf.split(Data, Data_y):\n",
    "                trainX, testX = Data[train_index], Data[test_index]\n",
    "                trainM, testM = Missing[train_index], Missing[test_index]\n",
    "                trainY, testY = Data_y[train_index] , Data_y[test_index]\n",
    "\n",
    "                if use_gpu is True:\n",
    "                    netD = NetD(Dim, H_Dim1, H_Dim2).cuda()\n",
    "                    netG = NetG(Dim, H_Dim1, H_Dim2).cuda()\n",
    "                else:\n",
    "                    netD = NetD(Dim, H_Dim1, H_Dim2)\n",
    "                    netG = NetG(Dim, H_Dim1, H_Dim2)\n",
    "\n",
    "                optimD = torch.optim.Adam(netD.parameters(), lr=learning_rate) # discriminator optimizer\n",
    "                optimG = torch.optim.Adam(netG.parameters(), lr=learning_rate) # generator optimizer\n",
    "\n",
    "                trainGAIN(netG, netD, Dim, trainX, trainM, optimG, optimD, Lg=Lg, Lm=Lm, hint=hint, \n",
    "                          alpha=alpha, epochs=epochs, printLoss=printLoss)\n",
    "\n",
    "                # Impute train and test and then fit on logistic\n",
    "                trainX = testGAIN(netG, netD, Dim, trainX, trainM, impute=impute)\n",
    "                testX  = testGAIN(netG, netD, Dim, testX, testM, impute=impute)\n",
    "\n",
    "                # Train a logistic Regression\n",
    "                clf = LogisticRegression(random_state=0,solver='lbfgs', \n",
    "                                         max_iter=2000).fit(trainX.detach().numpy(),trainY)\n",
    "                preds = clf.predict(testX.detach().numpy())\n",
    "\n",
    "                # print(\"roc_auc_score: {}\".format(roc_auc_score(testY, preds)))\n",
    "                fpr, tpr, thresholds = roc_curve(testY, preds)\n",
    "                auroc_scores.append(auc(fpr, tpr))\n",
    "                \n",
    "            auroc_mean.append(np.mean(auroc_scores))\n",
    "    \n",
    "    if impute==False:\n",
    "        total_avg = sum(experiment_rmse_mean) / len(experiment_rmse_mean)\n",
    "        total_std = np.sqrt(sum((rmse - total_avg )**2 for rmse in experiment_rmse_mean)/len(experiment_rmse_mean))\n",
    "        print(\"Total average: {}\".format(total_avg))\n",
    "        print(\"Total std: {}\".format(total_std))\n",
    "        return total_avg, total_std\n",
    "    else:\n",
    "        return auroc_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain_Loss_Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets (without labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Data (without labels)\n",
    "dataset_file_list = ['data_no_labels/breast.csv', 'data_no_labels/Spam.csv', 'data_no_labels/Letter.csv', \n",
    "                     'data_no_labels/credit.csv', 'data_no_labels/OnlineNewsPopularity.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments \n",
    "\n",
    "#### RMSE without Cross Validation\n",
    "\n",
    "Using all loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_file in dataset_file_list:\n",
    "    print('Dataset: {}'.format(str(dataset_file)))\n",
    "    # %% Initializations    \n",
    "    Data, No, Dim, H_Dim1, H_Dim2, Min_Val, Max_Val, p_miss_vec, Missing, idx, Train_No, Test_No = initializations(dataset_file)\n",
    "    \n",
    "    # Train / Test Features\n",
    "    trainX = Data[idx[:Train_No],:]\n",
    "    testX = Data[idx[Train_No:],:]\n",
    "    \n",
    "    # Train / Test Missing Indicators\n",
    "    trainM = Missing[idx[:Train_No],:]\n",
    "    testM = Missing[idx[Train_No:],:]\n",
    "    \n",
    "    # %% Init Network \n",
    "    if use_gpu is True:\n",
    "        netD = NetD(Dim, H_Dim1, H_Dim2).cuda()\n",
    "        netG = NetG(Dim, H_Dim1, H_Dim2).cuda()\n",
    "    else:\n",
    "        netD = NetD(Dim, H_Dim1, H_Dim2)\n",
    "        netG = NetG(Dim, H_Dim1, H_Dim2)\n",
    "  \n",
    "    # Optimizers\n",
    "    optimD = torch.optim.Adam(netD.parameters(), lr=learning_rate) # discriminator optimizer\n",
    "    optimG = torch.optim.Adam(netG.parameters(), lr=learning_rate) # generator optimizer\n",
    "    \n",
    "    # %% Training\n",
    "    trainGAIN(netG, netD, Dim, trainX, trainM, optimG, optimD, cv=False, Lg=True, Lm=True, hint=True, \n",
    "              alpha=alpha, epochs=epochs)\n",
    "\n",
    "    # %% Testing\n",
    "    rmse_score = testGAIN(netG, netD, Dim, testM, testX, cv=False)\n",
    "    print('Final Test RMSE: ' + str(rmse_score))\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE avg +/- std 2 times on 5 Fold Cross Validation\n",
    "\n",
    "Using all loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_avg_list_1 = []\n",
    "total_std_list_1 = []\n",
    "for dataset_file in dataset_file_list:\n",
    "    print('Dataset: {}'.format(str(dataset_file)))\n",
    "    total_avg, total_std = experiment(dataset_file, Lg=True, Lm=True, hint=True, \n",
    "                                      epochs=epochs, learning_rate=learning_rate, printLoss=False)\n",
    "    total_avg_list_1.append(total_avg)\n",
    "    total_std_list_1.append(total_std)\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_avg_list_1)\n",
    "print(total_std_list_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE avg +- std 2 times on 5 fold cross validation\n",
    "\n",
    "Without Lg (generator's loss ) = cross_entropy(discriminator's output and mask) + \n",
    "\n",
    "We use only the below equation to Update Generator:\n",
    "\n",
    "    alpha*mse(x_missing_data,x_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_avg_list_2 = []\n",
    "total_std_list_2 = []\n",
    "for dataset_file in dataset_file_list:\n",
    "    print('Dataset: {}'.format(str(dataset_file)))\n",
    "    total_avg, total_std = experiment(dataset_file, Lg=False, Lm=True, hint=True, \n",
    "                                      epochs=epochs, learning_rate=learning_rate, printLoss=False)\n",
    "    total_avg_list_2.append(total_avg)\n",
    "    total_std_list_2.append(total_std)\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"breast | spam | letter | credit | news\")\n",
    "print(total_avg_list_2)\n",
    "print(total_std_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE avg +- std 2 times on 5 Fold Cross Validation\n",
    "\n",
    "Without Lm = alpha*mse(x_missing_data,x_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_avg_list_3 = []\n",
    "total_std_list_3 = []\n",
    "for dataset_file in dataset_file_list:\n",
    "    print('Dataset: {}'.format(str(dataset_file)))\n",
    "    total_avg, total_std = experiment(dataset_file, Lg=True, Lm=False, hint=True, \n",
    "                                      epochs=epochs, learning_rate=learning_rate, printLoss=False)\n",
    "    total_avg_list_3.append(total_avg)\n",
    "    total_std_list_3.append(total_std)\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"breast | spam | letter | credit | news\")\n",
    "print(total_avg_list_3)\n",
    "print(total_std_list_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE avg +- std 2 times on 5 Fold Cross Validation\n",
    "\n",
    "Without Hint Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_avg_list_4 = []\n",
    "total_std_list_4 = []\n",
    "for dataset_file in dataset_file_list:\n",
    "    print('Dataset: {}'.format(str(dataset_file)))\n",
    "    total_avg, total_std = experiment(dataset_file, Lg=True, Lm=True, hint=False, \n",
    "                                      epochs=epochs, learning_rate=learning_rate, printLoss=False)\n",
    "    total_avg_list_4.append(total_avg)\n",
    "    total_std_list_4.append(total_std)\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"breast | spam | letter | credit | news\")\n",
    "print(total_avg_list_4)\n",
    "print(total_std_list_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE avg +- std 2 times on 5 Fold Cross Validation\n",
    "\n",
    "Without Hint and Lm loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_avg_list_5 = []\n",
    "total_std_list_5 = []\n",
    "for dataset_file in dataset_file_list:\n",
    "    print('Dataset: {}'.format(str(dataset_file)))\n",
    "    total_avg, total_std = experiment(dataset_file, Lg=True, Lm=False, hint=False, \n",
    "                                      epochs=epochs, learning_rate=learning_rate, printLoss=False)\n",
    "    total_avg_list_5.append(total_avg)\n",
    "    total_std_list_5.append(total_std)\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"breast | spam | letter | credit | news\")\n",
    "print(total_avg_list_5)\n",
    "print(total_std_list_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain_plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting RMSE VS Missing Data ,  RMSE VS #samples , RMSE vs #features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('plots/gain_td/'):\n",
    "    os.makedirs('plots/gain_td/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot for Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset System Parameters\n",
    "mb_size, p_miss, p_hint, alpha, train_rate, learning_rate, epochs = init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rates = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "total_6 = []\n",
    "for dataset_file in dataset_file_list:\n",
    "    print('Dataset: {}'.format(str(dataset_file)))\n",
    "    total_avg_list_6 = []\n",
    "    for mr in missing_rates:\n",
    "        # Missing rate\n",
    "        p_miss = mr\n",
    "        total_avg, total_std = experiment(dataset_file, Lg=True, Lm=True, hint=True, \n",
    "                                          epochs=epochs, learning_rate=learning_rate, printLoss=False)\n",
    "        total_avg_list_6.append(total_avg)\n",
    "    total_6.append(total_avg_list_6)\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"breast | spam | letter | credit | news\")\n",
    "print(total_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(missing_rates, total_6[0], total_6[1], \n",
    "                           total_6[2], total_6[3], total_6[4])), \n",
    "                  columns =['Missing rate', 'breast', 'spam', 'letter', 'credit', 'news'])\n",
    "\n",
    "df = df.melt('Missing rate', var_name='Datasets', value_name='RMSE')\n",
    "# g = sns.catplot(x='Missing rate %', y=\"RMSE\", hue='Datasets', data=df, kind='point')\n",
    "sns.catplot(x='Missing rate', y=\"RMSE\", hue='Datasets', data=df, kind='point').savefig(\"plots/gain_td/missing.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot with Data Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset System Parameters\n",
    "mb_size, p_miss, p_hint, alpha, train_rate, learning_rate, epochs = init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_7 = []\n",
    "data_sizes = [0.1, 0.3, 0.5, 0.7]\n",
    "for dataset_file in dataset_file_list:\n",
    "    total_avg_list_7 = []\n",
    "    print('Dataset: {}'.format(str(dataset_file)))\n",
    "    \n",
    "    if (str(dataset_file)=='data_no_labels/breast.csv'): ########\n",
    "        mb_size = 28\n",
    "    else:\n",
    "        mb_size = 64\n",
    "        \n",
    "    Data_full = np.loadtxt(dataset_file, delimiter=\",\",skiprows=1)\n",
    "    data_size = [int(data_sizes[0]*len(Data_full)),int(data_sizes[1]*len(Data_full)),\n",
    "                 int(data_sizes[2]*len(Data_full)),int(data_sizes[3]*len(Data_full))]\n",
    "    for lnth in data_size:\n",
    "        total_avg, total_std = experiment(dataset_file, Lg=True, Lm=True, hint=True, epochs=epochs, \n",
    "                                          learning_rate=learning_rate, size=True, lnth=lnth, printLoss=False)\n",
    "        total_avg_list_7.append(total_avg)\n",
    "    total_7.append(total_avg_list_7)\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"breast | spam | letter | credit | news\")\n",
    "print(total_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = [0.1, 0.3, 0.5, 0.7]\n",
    "df = pd.DataFrame(list(zip(data_sizes, total_7[0], total_7[1], total_7[2], total_7[3], total_7[4])), \n",
    "               columns =['Data Size', 'breast', 'spam', 'letter', 'credit', 'news'])\n",
    "\n",
    "df = df.melt('Data Size', var_name='Datasets', value_name='RMSE')\n",
    "# g = sns.catplot(x='Data Size %', y=\"RMSE\", hue='Datasets', data=df, kind='point')\n",
    "sns.catplot(x='Data Size', y=\"RMSE\", hue='Datasets', data=df, kind='point').savefig(\"plots/gain_td/datasize.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot for Alpha hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset System Parameters\n",
    "mb_size, p_miss, p_hint, alpha, train_rate, learning_rate, epochs = init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [0, 3, 5, 6, 8, 10, 15, 20]\n",
    "\n",
    "total_8 = []\n",
    "for dataset_file in dataset_file_list:\n",
    "    total_avg_list_8 = []\n",
    "    print('Dataset: {}'.format(str(dataset_file)))\n",
    "    for a in alpha_list:\n",
    "        total_avg, total_std = experiment(dataset_file, Lg=True, Lm=True, hint=True, \n",
    "                                          epochs=epochs, learning_rate=learning_rate, alpha=a, printLoss=False)\n",
    "        total_avg_list_8.append(total_avg)\n",
    "    total_8.append(total_avg_list_8)\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"breast | spam | letter | credit | news\")\n",
    "print(total_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(alpha_list, total_8[0], total_8[1], \n",
    "                           total_8[2], total_8[3], total_8[4])), \n",
    "                  columns =['alpha hyperparameter', 'breast', 'spam', 'letter', 'credit', 'news'])\n",
    "\n",
    "df = df.melt('alpha hyperparameter', var_name='Datasets', value_name='RMSE')\n",
    "# g = sns.catplot(x='alpha hyperparameter', y=\"RMSE\", hue='Datasets', data=df, kind='point')\n",
    "sns.catplot(x='alpha hyperparameter', y=\"RMSE\", hue='Datasets', data=df, kind='point').savefig(\"plots/gain_td/alpha.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot for Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset System Parameters\n",
    "mb_size, p_miss, p_hint, alpha, train_rate, learning_rate, epochs = init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = [0.1, 0.01, 0.001]\n",
    "\n",
    "total_9 = []\n",
    "for dataset_file in dataset_file_list:\n",
    "    total_avg_list_9 = []\n",
    "    print('Dataset: {}'.format(str(dataset_file)))\n",
    "    for lr in learning_rate_list:\n",
    "        total_avg, total_std = experiment(dataset_file, Lg=True, Lm=True, hint=True, \n",
    "                                          epochs=epochs, learning_rate=lr, printLoss=False)\n",
    "        total_avg_list_9.append(total_avg)\n",
    "    total_9.append(total_avg_list_9)\n",
    "    print(\"-------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"breast | spam | letter | credit | news\")\n",
    "print(total_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(learning_rate_list, total_9[0], total_9[1], \n",
    "                           total_9[2], total_9[3], total_9[4])), \n",
    "                  columns =['learning rate', 'breast', 'spam', 'letter', 'credit', 'news'])\n",
    "\n",
    "df = df.melt('learning rate', var_name='Datasets', value_name='RMSE')\n",
    "# g = sns.catplot(x='learning rate', y=\"RMSE\", hue='Datasets', data=df, kind='point')\n",
    "sns.catplot(x='learning rate', y=\"RMSE\", hue='Datasets', data=df, kind='point').savefig(\"plots/gain_td/lrates.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain_AUROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets (with labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Data (with labels)\n",
    "dataset_file_list_labels = ['data_with_labels/breast_with_label.csv', 'data_with_labels/Spam_with_label.csv', \n",
    "                            'data_with_labels/credit_with_label.csv', \n",
    "                            'data_with_labels/OnlineNewsPopularity_with_label.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting missing data over auroc score\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset System Parameters\n",
    "mb_size, p_miss, p_hint, alpha, train_rate, learning_rate, epochs = init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "auroc_mean_list = []\n",
    "\n",
    "for dataset_file in dataset_file_list_labels:\n",
    "    print('Dataset: {}'.format(str(dataset_file)))\n",
    "    auroc_mean = []\n",
    "    for mr in missing_rates:\n",
    "        # Missing rate\n",
    "        p_miss = mr\n",
    "        auroc_mean.append(experiment(dataset_file, Lg=True, Lm=True, hint=True, \n",
    "                                     epochs=epochs, learning_rate=learning_rate, impute=True, printLoss=False)) \n",
    "    auroc_mean_list.append(auroc_mean)\n",
    "    print(\"-------------------------------------\")\n",
    "    print()\n",
    "\n",
    "auroc_mean_ = np.array(auroc_mean_list)\n",
    "auroc_mean_ = auroc_mean_.transpose(2, 0, 1).reshape(-1, auroc_mean_.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"breast | spam | credit | news\")\n",
    "print(auroc_mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(missing_rates, list(auroc_mean_[0]), list(auroc_mean_[1]), \n",
    "                           list(auroc_mean_[2]) ,list(auroc_mean_[3]))), \n",
    "                  columns =['Missing Rate', 'breast', 'spam', 'credit', 'news'])\n",
    "\n",
    "df = df.melt('Missing Rate', var_name='Datasets', value_name='AUC score')\n",
    "# g = sns.catplot(x='Missing Rate %', y=\"AUC score\", hue='Datasets', data=df, kind='point')\n",
    "sns.catplot(x='Missing Rate', y=\"AUC score\", hue='Datasets', data=df, kind='point').savefig(\"plots/gain_td/auroc.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
