{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "National Technical University of Athens  \n",
    "School of Electrical and Computer Engineering   \n",
    "Data Science and Machine Learning   \n",
    "  \n",
    "\"Deep Learning Project\"  \n",
    "   \n",
    "Creators:  \n",
    "Zerkelidis Dimitris   \n",
    "Kaiktzoglou Maria  \n",
    "Trivyza Marilia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAIN: Missing Data Imputation using Generative Adversarial Nets\n",
    "\n",
    "Reference: Jinsung Yoon, James Jordon, and Mihaela van der Schaar, \"GAIN: Missing Data Imputation using Generative Adversarial Nets\", In International Conference on Machine Learning (ICML), 2018.  \n",
    "\n",
    "Adapted from the original implementation in tensorflow: https://github.com/jsyoon0823/GAIN\n",
    "\n",
    "#### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "old_v = tf.compat.v1.logging.get_verbosity()\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from input_data import read_data_sets\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True # set it to True to use GPU and False to use CPU\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN-FAST (Missing Rate: 0.5, sample_Z: high: 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Parameters\n",
    "\n",
    "Mini batch size: 128    \n",
    "Missing rate: 0.5   \n",
    "Hint rate: 0.9    \n",
    "Loss Hyperparameters: 10  \n",
    "  \n",
    "Input dimension (Fixed): 784   \n",
    "1st Hidden Layer Dim: 256     \n",
    "2nd Hidden Layer Dim: 128  \n",
    "Train_No: 55000    \n",
    "Test_No: 10000    \n",
    "  \n",
    "Learning Rate: 0.001    \n",
    "Epochs: 10000     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Initialize System Parameters\n",
    "def init_params(mb_size=128, p_miss=0.5, p_hint=0.9, alpha=10, \n",
    "                Train_No=55000, Test_No=10000, \n",
    "                Dim=784, H_Dim1=256, H_Dim2=128, \n",
    "                learning_rate=0.001, epochs=10000): \n",
    "    \n",
    "    # Mini batch size\n",
    "    mb_size = mb_size\n",
    "    # Missing rate\n",
    "    p_miss = p_miss\n",
    "    # Hint rate\n",
    "    p_hint = p_hint\n",
    "    # Loss Hyperparameters\n",
    "    alpha = alpha\n",
    "    \n",
    "    # No\n",
    "    Train_No = Train_No\n",
    "    Test_No = Test_No\n",
    "    \n",
    "    # Input dimension (Fixed)\n",
    "    Dim = Dim\n",
    "    # Hidden state dimensions\n",
    "    H_Dim1 = H_Dim1\n",
    "    H_Dim2 = H_Dim2\n",
    "    \n",
    "    # Learning Rate\n",
    "    learning_rate = learning_rate\n",
    "    # Epochs\n",
    "    epochs = epochs\n",
    "    \n",
    "    return mb_size, p_miss, p_hint, alpha, Dim, H_Dim1, H_Dim2, Train_No, Test_No, learning_rate, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size, p_miss, p_hint, alpha, Dim, H_Dim1, H_Dim2, Train_No, Test_No, learning_rate, epochs = init_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN Architecture   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetD, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(Dim * 2, H_Dim1)\n",
    "        self.fc2 = torch.nn.Linear(H_Dim1, H_Dim2)\n",
    "        self.fc3 = torch.nn.Linear(H_Dim2, Dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        layers = [self.fc1, self.fc2, self.fc3]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        inp = torch.cat((x, h), dim=1)\n",
    "        out = self.relu(self.fc1(inp))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.sigmoid(self.fc3(out)) # [0,1] Probability Output\n",
    "        return out\n",
    "\n",
    "    \n",
    "class NetG(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetG, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(Dim * 2, H_Dim1)\n",
    "        self.fc2 = torch.nn.Linear(H_Dim1, H_Dim2)\n",
    "        self.fc3 = torch.nn.Linear(H_Dim2, Dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        layers = [self.fc1, self.fc2, self.fc3]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "    def forward(self, x, m):\n",
    "        inp = torch.cat((x, m), dim=1)\n",
    "        out = self.relu(self.fc1(inp))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.sigmoid(self.fc3(out))  # [0,1] Probability Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask Vector and Hint Vector Generation\n",
    "def sample_M(m, n, p):\n",
    "    A = np.random.uniform(0., 1., size=[m, n])\n",
    "    B = A > p\n",
    "    C = 1. * B\n",
    "    return C\n",
    "\n",
    "\n",
    "# Random sample generator for Z\n",
    "def sample_Z(m, n, high):\n",
    "    return np.random.uniform(0., high, size=[m, n])\n",
    "\n",
    "\n",
    "# Mini-batch generation\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(netG, netD, M, X, H): \n",
    "    # Generator\n",
    "    G_sample = netG(X, M)\n",
    "    # Combine with original data\n",
    "    Hat_New_X = X * M + G_sample * (1-M)\n",
    "    # Discriminator\n",
    "    D_prob = netD(Hat_New_X, H)\n",
    "    # Loss\n",
    "    D_loss = -torch.mean(M * torch.log(D_prob + 1e-8) + (1-M) * torch.log(1. - D_prob + 1e-8))\n",
    "    return D_loss\n",
    "\n",
    "\n",
    "def generator_loss(netG, netD, X, M, New_X, H):\n",
    "    # %% Structure\n",
    "    # Generator\n",
    "    G_sample = netG(New_X, M)\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "    # Discriminator\n",
    "    D_prob = netD(Hat_New_X, H)\n",
    "\n",
    "    # Loss\n",
    "    G_loss1 = -torch.mean((1-M) * torch.log(D_prob + 1e-8))\n",
    "    MSE_train_loss = torch.mean((M * New_X - M * G_sample)**2) / torch.mean(M)\n",
    "    G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "\n",
    "    # MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return G_loss, MSE_train_loss, MSE_test_loss\n",
    "\n",
    "\n",
    "def test_loss(netG, netD, X, M, New_X):\n",
    "    # %% Structure\n",
    "    # Generator\n",
    "    G_sample = netG(New_X, M)\n",
    "\n",
    "    # MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return MSE_test_loss, G_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot (4 x 4 subfigures)\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    gs = gridspec.GridSpec(5, 5)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def outputFigure(it, netG, Test_No, mb_idx, Dim, high):\n",
    "    mb_idx = sample_idx(Test_No, 5)\n",
    "    X_mb = testX[mb_idx, :]\n",
    "    M_mb = testM[mb_idx, :]\n",
    "    Z_mb = sample_Z(5, Dim, high)\n",
    "\n",
    "    New_X_mb = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
    "\n",
    "    if use_gpu is True:\n",
    "        X_mb = torch.tensor(X_mb, device='cuda').float()\n",
    "        M_mb = torch.tensor(M_mb, device='cuda').float()\n",
    "        New_X_mb = torch.tensor(New_X_mb, device='cuda').float()\n",
    "        Z_mb = torch.tensor(Z_mb, device='cuda').float()\n",
    "    else:\n",
    "        X_mb = torch.tensor(X_mb).float()\n",
    "        M_mb = torch.tensor(M_mb).float()\n",
    "        New_X_mb = torch.tensor(New_X_mb).float()\n",
    "        Z_mb = torch.tensor(Z_mb).float()\n",
    "\n",
    "    samples1 = X_mb\n",
    "    samples5 = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
    "    inp = M_mb * X_mb + (1-M_mb) * New_X_mb\n",
    "    samples2 = netG(inp, M_mb)\n",
    "    samples2 = M_mb * X_mb + (1 - M_mb) * samples2\n",
    "\n",
    "    Z_mb = sample_Z(5, Dim, high)\n",
    "    if use_gpu is True:\n",
    "        Z_mb = torch.tensor(Z_mb, device='cuda').float()\n",
    "    else:\n",
    "        Z_mb = torch.tensor(Z_mb).float()\n",
    "    New_X_mb = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
    "\n",
    "    inp = M_mb * X_mb + (1-M_mb) * New_X_mb\n",
    "    samples3 = netG(inp, M_mb)\n",
    "    samples3 = M_mb * X_mb + (1 - M_mb) * samples3\n",
    "\n",
    "    Z_mb = sample_Z(5, Dim, high)\n",
    "    if use_gpu is True:\n",
    "        Z_mb = torch.tensor(Z_mb, device='cuda').float()\n",
    "    else:\n",
    "        Z_mb = torch.tensor(Z_mb).float()\n",
    "    New_X_mb = M_mb * X_mb + (1 - M_mb) * Z_mb\n",
    "    inp = M_mb * X_mb + (1-M_mb) * New_X_mb\n",
    "    samples4 = netG(inp, M_mb)\n",
    "    samples4 = M_mb * X_mb + (1 - M_mb) * samples4\n",
    "\n",
    "    if use_gpu is True:\n",
    "        samples = np.vstack([samples5.cpu().detach().data, samples2.cpu().detach().data, \n",
    "                             samples3.cpu().detach().data, samples4.cpu().detach().data, \n",
    "                             samples1.cpu().detach().data])\n",
    "    else:\n",
    "        samples = np.vstack([samples5.detach().data, samples2.detach().data, \n",
    "                             samples3.detach().data, samples4.detach().data, \n",
    "                             samples1.detach().data])\n",
    "    fig = plot(samples)\n",
    "    plt.savefig('Multiple_Impute/{}.png'.format(str(it).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAIN(netG, netD, Dim, trainX, trainM, optimG, optimD, alpha=alpha, epochs=epochs, \n",
    "              high=0.01, printLoss=True, outFigure=True):\n",
    "    # %% Output Initialization\n",
    "    if outFigure==True:    \n",
    "        if not os.path.exists('Multiple_Impute/'):\n",
    "            os.makedirs('Multiple_Impute/')\n",
    "    \n",
    "    # %% Start Iterations\n",
    "    for it in tqdm(range(epochs)):\n",
    "        # %% Inputs\n",
    "        mb_idx = sample_idx(Train_No, mb_size) \n",
    "        X_mb = trainX[mb_idx,:]  \n",
    "        Z_mb = sample_Z(mb_size, Dim, high) \n",
    "\n",
    "        M_mb = trainM[mb_idx, :]     \n",
    "        H_mb1 = sample_M(mb_size, Dim, 1-p_hint) \n",
    "        H_mb = M_mb * H_mb1  \n",
    "        # H_mb = M_mb * H_mb1 + 0.5 * (1 - H_mb1)\n",
    "\n",
    "        New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce \n",
    "\n",
    "        if use_gpu is True:\n",
    "            X_mb = torch.tensor(X_mb, device='cuda').float()\n",
    "            M_mb = torch.tensor(M_mb, device='cuda').float()\n",
    "            H_mb = torch.tensor(H_mb, device='cuda').float()\n",
    "            New_X_mb = torch.tensor(New_X_mb, device='cuda').float()\n",
    "            Z_mb = torch.tensor(Z_mb, device='cuda').float()\n",
    "        else:\n",
    "            X_mb = torch.tensor(X_mb).float()\n",
    "            M_mb = torch.tensor(M_mb).float()\n",
    "            H_mb = torch.tensor(H_mb).float()\n",
    "            New_X_mb = torch.tensor(New_X_mb).float()\n",
    "            Z_mb = torch.tensor(Z_mb).float()\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimD.zero_grad() \n",
    "        D_loss = discriminator_loss(netG, netD, M=M_mb, X=New_X_mb, H=H_mb)\n",
    "        D_loss.backward()\n",
    "        optimD.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimG.zero_grad() \n",
    "        G_loss, G_mse_loss, G_mse_test = generator_loss(netG, netD, X=X_mb, M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "        G_loss.backward()\n",
    "        optimG.step()\n",
    "       \n",
    "        if it % 100 == 0:\n",
    "            # %% Intermediate Losses\n",
    "            if printLoss==True:\n",
    "                print('Iter: {}'.format(it),end='\\t')\n",
    "                print('Train_loss: {:.4}'.format(G_mse_loss),end='\\t')\n",
    "                print('Test_loss: {:.4}'.format(G_mse_test),end='\\t')\n",
    "                print('D_loss: {:.4}'.format(D_loss))\n",
    "            \n",
    "            # %% Output figure\n",
    "            if outFigure==True:\n",
    "                outputFigure(it, netG, Test_No, mb_idx, Dim, high) \n",
    "            \n",
    "\n",
    "def testGAIN(netG, netD, Dim, testM, testX, high=0.01):\n",
    "    Z_mb = sample_Z(Test_No, Dim, high) \n",
    "    M_mb = testM \n",
    "    X_mb = testX\n",
    "    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  \n",
    "\n",
    "    # make the tensors\n",
    "    if use_gpu is True:\n",
    "        X_mb = torch.tensor(X_mb, device='cuda').float()\n",
    "        M_mb = torch.tensor(M_mb, device='cuda').float()\n",
    "        New_X_mb = torch.tensor(New_X_mb, device='cuda').float()\n",
    "    else:\n",
    "        X_mb = torch.tensor(X_mb).float()\n",
    "        M_mb = torch.tensor(M_mb).float()\n",
    "        New_X_mb = torch.tensor(New_X_mb).float()\n",
    "\n",
    "    MSE_final, Sample = test_loss(netG, netD, X=X_mb, M=M_mb, New_X=New_X_mb)\n",
    "    print('Test RMSE: ' + str(np.sqrt(MSE_final.item())))\n",
    "    print()\n",
    "    \n",
    "    return Sample, New_X_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images before and after Imputation\n",
    "def plotBeforeAfter(New_X_mb, imputed_x, plot_name1, plot_name2):\n",
    "    if not os.path.exists('images/'):\n",
    "        os.makedirs('images/')\n",
    "    \n",
    "    # Plot images before Imputation\n",
    "    fig, axes = plt.subplots(10,10, figsize=(8,8))\n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        if use_gpu is True:\n",
    "            ax.imshow(New_X_mb[i].cpu().reshape(28,28))\n",
    "        else:\n",
    "            ax.imshow(New_X_mb[i].reshape(28,28))\n",
    "    fig.savefig('images/' + plot_name1)\n",
    "    \n",
    "    # Plot images after Imputation\n",
    "    fig, axes = plt.subplots(10,10, figsize=(8,8))\n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        if use_gpu is True:\n",
    "            ax.imshow(imputed_x[i].cpu().detach().numpy().reshape(28,28))\n",
    "        else:\n",
    "            ax.imshow(imputed_x[i].detach().numpy().reshape(28,28))\n",
    "    fig.savefig('images/' + plot_name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = input_data.read_data_sets('./MNIST_data', one_hot=True)\n",
    "mnist = read_data_sets('./MNIST_data', one_hot = True)\n",
    "tf.compat.v1.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Features\n",
    "trainX, _ = mnist.train.next_batch(Train_No)\n",
    "testX, _ = mnist.test.next_batch(Test_No)\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = sample_M(Train_No, Dim, p_miss)\n",
    "testM = sample_M(Test_No, Dim, p_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original images \n",
    "fig, axes = plt.subplots(10,10, figsize=(8,8))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    ax.imshow(testX[i].reshape(28,28))\n",
    "\n",
    "if not os.path.exists('images/'):\n",
    "    os.makedirs('images/')\n",
    "fig.savefig('images/' + 'original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Init Network \n",
    "if use_gpu is True:\n",
    "    netD = NetD().cuda()\n",
    "    netG = NetG().cuda()\n",
    "else:\n",
    "    netD = NetD()\n",
    "    netG = NetG()\n",
    "\n",
    "# Optimizers\n",
    "optimD = torch.optim.Adam(netD.parameters(), lr=learning_rate)\n",
    "optimG = torch.optim.Adam(netG.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Training\n",
    "trainGAIN(netG, netD, Dim, trainX, trainM, optimG, optimD, alpha=alpha, epochs=epochs, high=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Testing\n",
    "imputed_x, New_X_mb = testGAIN(netG, netD, Dim, testM, testX, high=1.)\n",
    "\n",
    "print(\"Imputed Data Shape:\", imputed_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot images before and after Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBeforeAfter(New_X_mb, imputed_x, 'noise_gain_fast_50.png', 'imputed_gain_fast_50.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### System Parameters\n",
    "\n",
    "Mini batch size: 128    \n",
    "Missing rate: 0.2   \n",
    "Hint rate: 0.9    \n",
    "Loss Hyperparameters: 10  \n",
    "  \n",
    "Input dimension (Fixed): 784    \n",
    "1st Hidden Layer Dim: 784      \n",
    "2nd Hidden Layer Dim: 784   \n",
    "Train_No: 55000     \n",
    "Test_No: 10000    \n",
    "   \n",
    "Learning Rate: 0.001    \n",
    "Epochs: 10000    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Parameters\n",
    "mb_size, p_miss, p_hint, alpha, Dim, H_Dim1, H_Dim2, Train_No, Test_No, learning_rate, epochs = init_params(mb_size=128, p_miss=0.2, p_hint=0.9, alpha=10, \n",
    "                                                                                                            Train_No=55000, Test_No=10000, \n",
    "                                                                                                            Dim=784, H_Dim1=784, H_Dim2=784, \n",
    "                                                                                                            learning_rate=0.001, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Init Network \n",
    "if use_gpu is True:\n",
    "    netD = NetD().cuda()\n",
    "    netG = NetG().cuda()\n",
    "else:\n",
    "    netD = NetD()\n",
    "    netG = NetG()\n",
    "\n",
    "# Optimizers\n",
    "optimD = torch.optim.Adam(netD.parameters(), lr=learning_rate)\n",
    "optimG = torch.optim.Adam(netG.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGAIN(netG, netD, Dim, trainX, trainM, optimG, optimD, alpha=alpha, epochs=epochs, \n",
    "          high=0.01, printLoss=False, outFigure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Testing\n",
    "imputed_x, New_X_mb = testGAIN(netG, netD, Dim, testM, testX)\n",
    "\n",
    "print(\"Imputed Data Shape:\", imputed_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBeforeAfter(New_X_mb, imputed_x, 'noise_gain.png', 'imputed_gain.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN-FAST (Missing Rate: 0.2, sample_Z: high: 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Parameters\n",
    "mb_size, p_miss, p_hint, alpha, Dim, H_Dim1, H_Dim2, Train_No, Test_No, learning_rate, epochs = init_params(p_miss=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Init Network \n",
    "if use_gpu is True:\n",
    "    netD = NetD().cuda()\n",
    "    netG = NetG().cuda()\n",
    "else:\n",
    "    netD = NetD()\n",
    "    netG = NetG()\n",
    "\n",
    "# Optimizers\n",
    "optimD = torch.optim.Adam(netD.parameters(), lr=learning_rate)\n",
    "optimG = torch.optim.Adam(netG.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGAIN(netG, netD, Dim, trainX, trainM, optimG, optimD, alpha=alpha, epochs=epochs, \n",
    "          high=0.01, printLoss=False, outFigure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Testing\n",
    "imputed_x, New_X_mb = testGAIN(netG, netD, Dim, testM, testX)\n",
    "\n",
    "print(\"Imputed Data Shape:\", imputed_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBeforeAfter(New_X_mb, imputed_x, 'noise_gain_fast.png', 'imputed_gain_fast.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Parameters\n",
    "mb_size, p_miss, p_hint, alpha, Dim, H_Dim1, H_Dim2, Train_No, Test_No, learning_rate, epochs = init_params(p_miss=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining cnn layers\n",
    "class encode(torch.nn.Module):\n",
    "    def __init__(self,in_c):\n",
    "        super(encode,self).__init__()\n",
    "        # encoder layers\n",
    "        # conv layer (depth from 1 --> 32), 3x3 kernels\n",
    "        self.conv1 = torch.nn.Conv2d(in_c, 32, 3, padding=1)  \n",
    "        # conv layer (depth from 32 --> 16), 3x3 kernels\n",
    "        self.conv2 = torch.nn.Conv2d(32, 16, 3, padding=1)\n",
    "        # conv layer (depth from 16 --> 8), 3x3 kernels\n",
    "        self.conv3 = torch.nn.Conv2d(16, 8, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encode \n",
    "        # add hidden layers with relu activation function and maxpooling after\n",
    "        x = F.relu(self.conv1(x.float()))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # add third hidden layer\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x) # compressed representation\n",
    "        return x\n",
    "    \n",
    "\n",
    "class decode(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decode,self).__init__()\n",
    "        self.t_conv1 = torch.nn.ConvTranspose2d(8, 8, 3, stride=2)  # kernel_size=3 to get to a 7x7 image       \n",
    "        self.t_conv2 = torch.nn.ConvTranspose2d(8, 16, 2, stride=2)\n",
    "        self.t_conv3 = torch.nn.ConvTranspose2d(16, 32, 2, stride=2)\n",
    "        # one, final, normal conv layer to decrease the depth\n",
    "        self.conv_out = torch.nn.Conv2d(32, 1, 3, padding=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        x = F.relu(self.t_conv3(x))\n",
    "        # transpose again, output should have a sigmoid applied\n",
    "        x = torch.sigmoid(self.conv_out(x))        \n",
    "        return x.reshape(-1, 28, 28).flatten(start_dim=1, end_dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetDCNN, self).__init__()\n",
    "        # ADD CNN LAYERS \n",
    "        # encode-decode\n",
    "        self.encode = encode(2)\n",
    "        self.decode = decode()\n",
    "        self.net = torch.nn.Sequential(self.encode, self.decode)\n",
    "        # keep as they are\n",
    "        self.fc3 = torch.nn.Linear(Dim, Dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "    \n",
    "    def init_weight(self):\n",
    "        layers = [self.fc3]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        inp, h = x.reshape(-1, 1, 28, 28), h.reshape(-1, 1, 28, 28)\n",
    "        inp = torch.cat((inp, h), dim=1)\n",
    "        # cnn-encode-decode\n",
    "        out = self.net(inp)\n",
    "        out = self.sigmoid(self.fc3(out))\n",
    "        return out    \n",
    "\n",
    "    \n",
    "class NetGCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetGCNN, self).__init__()\n",
    "        # ADD CNN LAYERS \n",
    "        # encode-decode\n",
    "        self.encode = encode(2)\n",
    "        self.decode = decode()\n",
    "        self.net = torch.nn.Sequential(self.encode, self.decode)\n",
    "        # keep as they are \n",
    "        self.fc3 = torch.nn.Linear(Dim, Dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid() # [0,1] Probability Output\n",
    "        self.init_weight()\n",
    "    \n",
    "    def init_weight(self):\n",
    "        layers = [self.fc3]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "        \n",
    "    def forward(self, x, m):\n",
    "        inp, m = x.reshape(-1, 1, 28, 28), m.reshape(-1, 1, 28, 28)\n",
    "        inp = torch.cat((inp, m), dim=1)\n",
    "        # Cnn Layers forward \n",
    "        out = self.net(inp)\n",
    "        # keep as it is \n",
    "        out = self.sigmoid(self.fc3(out)) # [0,1] Probability Output\n",
    "        return out     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Init Network \n",
    "if use_gpu is True:\n",
    "    netDCNN = NetDCNN().cuda()\n",
    "    netGCNN = NetGCNN().cuda()\n",
    "else:\n",
    "    netDCNN = NetDCNN()\n",
    "    netGCNN = NetGCNN()\n",
    "\n",
    "# Optimizers\n",
    "optimDCNN = torch.optim.Adam(netDCNN.parameters(), lr=learning_rate)\n",
    "optimGCNN = torch.optim.Adam(netGCNN.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGAIN(netGCNN, netDCNN, Dim, trainX, trainM, optimGCNN, optimDCNN, alpha=alpha, epochs=epochs, \n",
    "          high=0.01, printLoss=False, outFigure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Testing\n",
    "imputed_x, New_X_mb = testGAIN(netGCNN, netDCNN, Dim, testM, testX)\n",
    "\n",
    "print(\"Imputed Data Shape:\", imputed_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBeforeAfter(New_X_mb, imputed_x, 'noise_gain_cnn.png', 'imputed_gain_cnn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN-DEEP (adding 2 more hidden layers to the original architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size, p_miss, p_hint, alpha, Dim, H_Dim1, H_Dim2, Train_No, Test_No, learning_rate, epochs = init_params(p_miss=0.2, Dim=784, H_Dim1=784, H_Dim2=784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetD_deep(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetD_deep, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(Dim * 2, H_Dim1)\n",
    "        self.fc2 = torch.nn.Linear(H_Dim1, H_Dim1)\n",
    "        self.fc3 = torch.nn.Linear(H_Dim1, H_Dim2)\n",
    "        self.fc4 = torch.nn.Linear(H_Dim2, H_Dim2)\n",
    "        self.fc5 = torch.nn.Linear(H_Dim2, Dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        layers = [self.fc1, self.fc2, self.fc3, self.fc4, self.fc5]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        inp = torch.cat((x, h), dim=1)\n",
    "        out = self.relu(self.fc1(inp))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.sigmoid(self.fc5(out)) # [0,1] Probability Output\n",
    "        return out\n",
    "\n",
    "    \n",
    "class NetG_deep(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetG_deep, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(Dim * 2, H_Dim1)\n",
    "        self.fc2 = torch.nn.Linear(H_Dim1, H_Dim1)\n",
    "        self.fc3 = torch.nn.Linear(H_Dim1, H_Dim2)\n",
    "        self.fc4 = torch.nn.Linear(H_Dim2, H_Dim2)\n",
    "        self.fc5 = torch.nn.Linear(H_Dim2, Dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        layers = [self.fc1, self.fc2, self.fc3, self.fc4, self.fc5]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "    def forward(self, x, m):\n",
    "        inp = torch.cat((x, m), dim=1)\n",
    "        out = self.relu(self.fc1(inp))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.sigmoid(self.fc5(out))  # [0,1] Probability Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Init Network \n",
    "if use_gpu is True:\n",
    "    netD_deep = NetD_deep().cuda()\n",
    "    netG_deep = NetG_deep().cuda()\n",
    "else:\n",
    "    netD_deep = NetD_deep()\n",
    "    netG_deep = NetG_deep()\n",
    "\n",
    "# Optimizers\n",
    "optim_netD_deep = torch.optim.Adam(netD_deep.parameters(), lr=learning_rate)\n",
    "optim_netG_deep = torch.optim.Adam(netG_deep.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGAIN(netG_deep, netD_deep, Dim, trainX, trainM, optim_netG_deep, optim_netD_deep, alpha=alpha, epochs=epochs, \n",
    "          high=0.01, printLoss=False, outFigure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Testing\n",
    "imputed_x, New_X_mb = testGAIN(netG_deep, netD_deep, Dim, testM, testX)\n",
    "\n",
    "print(\"Imputed Data Shape:\", imputed_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBeforeAfter(New_X_mb, imputed_x, 'noise_gain_deep.png', 'imputed_gain_deep.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAIN-TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size, p_miss, p_hint, alpha, Dim, H_Dim1, H_Dim2, Train_No, Test_No, learning_rate, epochs = init_params(mb_size=64, \n",
    "                                                                                                            p_miss=0.2, \n",
    "                                                                                                            Dim=784, \n",
    "                                                                                                            H_Dim1=784, \n",
    "                                                                                                            H_Dim2=784//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetD_td(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetD_td, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(Dim * 2, H_Dim1) # with Hint\n",
    "        self.fc2 = torch.nn.Linear(H_Dim1, H_Dim2)\n",
    "        self.fc3 = torch.nn.Linear(H_Dim2, H_Dim1)\n",
    "        self.fc4 = torch.nn.Linear(H_Dim1, Dim)\n",
    "        \n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        layers = [self.fc1, self.fc2, self.fc3, self.fc4]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        inp = torch.cat((x, h), dim=1)\n",
    "        out = self.tanh(self.fc1(inp))\n",
    "        out = self.tanh(self.fc2(out))\n",
    "        out = self.tanh(self.fc3(out))\n",
    "        out = self.sigmoid(self.fc4(out)) # [0,1] Probability Output\n",
    "        return out\n",
    "\n",
    "    \n",
    "class NetG_td(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetG_td, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(Dim * 2, H_Dim1) # with Hint\n",
    "        self.fc2 = torch.nn.Linear(H_Dim1, H_Dim2)\n",
    "        self.fc3 = torch.nn.Linear(H_Dim2, H_Dim1)\n",
    "        self.fc4 = torch.nn.Linear(H_Dim1, Dim)\n",
    "        \n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        layers = [self.fc1, self.fc2, self.fc3, self.fc4]\n",
    "        [torch.nn.init.xavier_normal_(layer.weight) for layer in layers]\n",
    "\n",
    "    def forward(self, x, m):\n",
    "        inp = torch.cat((x, m), dim=1)\n",
    "        out = self.tanh(self.fc1(inp))\n",
    "        out = self.tanh(self.fc2(out))\n",
    "        out = self.tanh(self.fc3(out))\n",
    "        out = self.sigmoid(self.fc4(out))  # [0,1] Probability Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Init Network \n",
    "if use_gpu is True:\n",
    "    netD_td = NetD_td().cuda()\n",
    "    netG_td = NetG_td().cuda()\n",
    "else:\n",
    "    netD_td = NetD_td()\n",
    "    netG_td = NetG_td()\n",
    "\n",
    "# Optimizers\n",
    "optim_netD_td = torch.optim.Adam(netD_td.parameters(), lr=learning_rate)\n",
    "optim_netG_td = torch.optim.Adam(netG_td.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGAIN(netG_td, netD_td, Dim, trainX, trainM, optim_netG_td, optim_netD_td, alpha=alpha, epochs=epochs, \n",
    "          high=0.01, printLoss=False, outFigure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Testing\n",
    "imputed_x, New_X_mb = testGAIN(netG_td, netD_td, Dim, testM, testX)\n",
    "\n",
    "print(\"Imputed Data Shape:\", imputed_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBeforeAfter(New_X_mb, imputed_x, 'noise_gain_td.png', 'imputed_gain_td.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
